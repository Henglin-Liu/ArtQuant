{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_prompts = [\n",
    "    'Rate the quality of this painting.\\n<|image|>',\n",
    "    'How would you rate the quality of this painting?\\n<|image|>',\n",
    "    'How would you judge the quality of this painting?\\n<|image|>',\n",
    "    'How do you assess the quality of this painting?\\n<|image|>',\n",
    "    'Could you evaluate the quality of this painting?\\n<|image|>',\n",
    "    'What is your quality rating for this painting?\\n<|image|>',\n",
    "    'Can you rate the quality of this painting?\\n<|image|>',\n",
    "    'What do you think about the quality of this painting?\\n<|image|>',\n",
    "    'Can you judge the quality of this painting?\\n<|image|>',\n",
    "    \"What's your opinion on the quality of this painting?\\n<|image|>\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Score data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_data = pd.read_csv('../dataset/APDD/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $l_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_value = [10.,7.5,5,3.5,1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_level(score,pdf_value):\n",
    "    if score <= pdf_value[-1]:\n",
    "        return 'bad'\n",
    "    elif score <= pdf_value[-2]:\n",
    "        return 'poor'\n",
    "    elif score <= pdf_value[-3]:\n",
    "        return 'fair'\n",
    "    elif score <= pdf_value[-4]:\n",
    "        return 'good'\n",
    "    else:\n",
    "        return 'excellent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mu_sigma(P_target):\n",
    "\n",
    "    # Define the objective function to minimize the error\n",
    "    def objective(params):\n",
    "        mu, sigma, = params\n",
    "        level_prob = norm.pdf(pdf_value, loc=mu, scale=sigma)\n",
    "        level_prob = level_prob/level_prob.sum()\n",
    "        prob_discrete = np.inner(level_prob, np.array(pdf_value))\n",
    "        \n",
    "        # Return the gap from the target probability\n",
    "        return abs(prob_discrete - P_target)\n",
    "\n",
    "    # Initial guess\n",
    "    initial_guess = [P_target, 1]\n",
    "    options = {\n",
    "        'disp': True,\n",
    "    }\n",
    "\n",
    "    # Use the minimization function of scipy to find the optimal mu and sigma\n",
    "    result = minimize(objective, initial_guess, bounds=[(-10, 10), (0.001, 1000)],\n",
    "                      options=options)\n",
    "\n",
    "    # Output the optimal mu and sigma\n",
    "    mu_optimal, sigma_optimal = result.x\n",
    "    return mu_optimal, sigma_optimal\n",
    "\n",
    "def get_score(score):\n",
    "    '''\n",
    "    Get the probability values for each rating level. The steps are as follows:\n",
    "        1. Calculate the mu and sigma corresponding to the most Gaussian distribution expressing the score.\n",
    "        2. Calculate the probability distribution for each rating level.\n",
    "        3. Normalize the probability distribution.\n",
    "        4. Return the normalized probability distribution.\n",
    "    '''\n",
    "    mu_optimal,sigma_optimal= get_mu_sigma(score)\n",
    "    level_prob = norm.pdf(pdf_value, loc=mu_optimal, scale=sigma_optimal)\n",
    "    level_prob = level_prob.clip(0,1)\n",
    "    return level_prob/level_prob.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/tmp/ipykernel_864251/2856257373.py:7: RuntimeWarning: invalid value encountered in divide\n",
      "  level_prob = level_prob/level_prob.sum()\n",
      "8019it [01:40, 79.70it/s] \n"
     ]
    }
   ],
   "source": [
    "dict_list = []\n",
    "# counter = 100\n",
    "for index, row in tqdm.tqdm(score_data.iterrows()):\n",
    "    filename = row['filename']\n",
    "    Score = row['Score']\n",
    "    \n",
    "    id = filename + '->' + str(Score)\n",
    "    image = filename\n",
    "    gt_score = Score\n",
    "    level_probs = get_score(Score)\n",
    "    level = get_level(Score)\n",
    "    \n",
    "    ask_index = np.random.randint(0, len(ask_prompts))\n",
    "    conversations = [{\"from\": \"human\", \"value\": ask_prompts[ask_index]},\n",
    "                     {\"from\": \"gpt\", \"value\": f\"The quality of the painting is {level}.\"}]\n",
    "\n",
    "    dict = {\n",
    "                \"id\": id, \n",
    "                \"image\": image,\n",
    "                \"gt_score\": gt_score, \n",
    "                \"level_probs\": level_probs, \n",
    "                \"conversations\": conversations\n",
    "            }\n",
    "    dict_list.append(dict)\n",
    "    \n",
    "    # counter -= 1\n",
    "    # if counter == 0:\n",
    "    #     break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# for d in dict_list:\n",
    "#     d['level_probs'] = list(d['level_probs'])\n",
    "\n",
    "# with open('dataset/APDD/score.json', 'w') as f:\n",
    "#     f.write(json.dumps(dict_list, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeQA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
